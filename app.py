import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import f1_score, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
import json
from fastapi import FastAPI
import uvicorn
from datetime import datetime, timedelta
from send_telegram import send_telegram_message # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–æ—Ç —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
import time
import optuna
import logging
import optuna.samplers
from optuna.integration import TFKerasPruningCallback # –î–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞ Keras –º–æ–¥–µ–ª–µ–π

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping

import config

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(level=config.LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI()

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ config.py ---
MODEL_PATH = config.MODEL_PATH
ACCURACY_PATH = config.ACCURACY_PATH
HORIZON_PERIODS = config.HORIZON_PERIODS
LOOKBACK_PERIOD = config.LOOKBACK_PERIOD
MIN_DATA_ROWS = config.MIN_DATA_ROWS
TARGET_ACCURACY = config.TARGET_ACCURACY
MIN_ACCURACY_FOR_SIGNAL = config.MIN_ACCURACY_FOR_SIGNAL
MAX_TRAINING_TIME = config.MAX_TRAINING_TIME
PREDICTION_PROB_THRESHOLD = config.PREDICTION_PROB_THRESHOLD
N_SPLITS_TS_CV = config.N_SPLITS_TS_CV
OPTUNA_STORAGE_URL = config.OPTUNA_STORAGE_URL
OPTUNA_STUDY_NAME = config.OPTUNA_STUDY_NAME
SEQUENCE_LENGTH = config.SEQUENCE_LENGTH
NN_EPOCHS = config.NN_EPOCHS
NN_BATCH_SIZE = config.NN_BATCH_SIZE
NN_PATIENCE = config.NN_PATIENCE

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
def compute_rsi(data, periods=14):
    delta = data.diff()
    gain = delta.where(delta > 0, 0).rolling(window=periods).mean()
    loss = -delta.where(delta < 0, 0).rolling(window=periods).mean()
    rs = gain / (loss + 1e-10)
    rsi = 100 - (100 / (1 + rs))
    return rsi

def compute_bollinger_bands(data, window=20, num_std=2):
    ma = data.rolling(window=window).mean()
    std = data.rolling(window=window).std()
    upper = ma + (num_std * std)
    lower = ma - (num_std * std)
    return upper, lower

def compute_macd(data, fast=12, slow=26, signal=9):
    exp1 = data.ewm(span=fast, adjust=False).mean()
    exp2 = data.ewm(span=slow, adjust=False).mean()
    macd = exp1 - exp2
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    return macd, signal_line

def compute_stochastic_oscillator(high, low, close, k_period=14, d_period=3):
    lowest_low = low.rolling(window=k_period).min()
    highest_high = high.rolling(window=k_period).max()
    k_percent = ((close - lowest_low) / (highest_high - lowest_low + 1e-10)) * 100
    d_percent = k_percent.rolling(window=d_period).mean()
    return k_percent, d_percent

def compute_atr(high, low, close, period=14):
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = true_range.ewm(span=period, adjust=False).mean()
    return atr

def compute_roc(data, period=12):
    roc = ((data - data.shift(period)) / data.shift(period)) * 100
    return roc

def create_sequences(features, target, sequence_length, horizon_periods):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
    
    Args:
        features (pd.DataFrame): DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.
        target (pd.Series): Series —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.
        sequence_length (int): –î–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è NN.
        horizon_periods (int): –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
        
    Returns:
        tuple: (np.array X_seq, np.array y_seq)
    """
    X_seq, y_seq = [], []
    # –î–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏: –æ—Ç –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ –∫–æ–Ω—Ü–∞,
    # —É—á–∏—Ç—ã–≤–∞—è –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
    for i in range(len(features) - sequence_length - horizon_periods + 1):
        # X_seq - —ç—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ `sequence_length` –±–∞—Ä–æ–≤
        X_seq.append(features.iloc[i:(i + sequence_length)].values)
        # y_seq - —ç—Ç–æ —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –±–∞—Ä—É —á–µ—Ä–µ–∑ `horizon_periods`
        # –æ—Ç –∫–æ–Ω—Ü–∞ —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        y_seq.append(target.iloc[i + sequence_length + horizon_periods - 1])
        
    return np.array(X_seq), np.array(y_seq)


def prepare_data():
    logger.info("Downloading data...")
    try:
        end_date = datetime.now()
        df = yf.download("EURUSD=X", interval="15m", period=LOOKBACK_PERIOD, end=end_date)
    except Exception as e:
        logger.error(f"Error downloading data: {str(e)}")
        raise ValueError(f"Error downloading data: {str(e)}")

    if df.empty:
        logger.error("Empty data received from Yahoo Finance.")
        raise ValueError("Empty data received from Yahoo Finance.")

    logger.info(f"Downloaded {len(df)} rows, from {df.index[0]} to {df.index[-1]} for 15m interval.")
    
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [col[0] for col in df.columns]

    df = df[(df['Close'] > 0) & (df['Open'] > 0)]
    logger.info(f"After filtering Open/Close > 0: {len(df)} rows")

    if 'Volume' in df.columns and df['Volume'].sum() == 0:
        logger.warning("Volume data is present but all values are zero. This is common for forex data from Yahoo Finance.")
    elif 'Volume' not in df.columns:
        logger.warning("Volume column not found in downloaded data. It will not be used as a feature.")
        df['Volume'] = 0

    df['Close'] = df['Close'].fillna(method='ffill').fillna(method='bfill')
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–µ–ø–µ—Ä—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ HORIZON_PERIODS –≤–ø–µ—Ä–µ–¥
    df['Target'] = df['Close'].shift(-HORIZON_PERIODS)

    if df['Target'].isna().all():
        logger.error("Target column contains only NaNs.")
        raise ValueError("Target column contains only NaNs.")

    df_features = df.copy() 

    df_features['RSI'] = compute_rsi(df_features['Close'])
    df_features['MA20'] = df_features['Close'].rolling(window=20).mean()
    df_features['BB_Up'], df_features['BB_Low'] = compute_bollinger_bands(df_features['Close'])
    df_features['MACD'], df_features['MACD_Sig'] = compute_macd(df_features['Close'])
    
    df_features['Stoch_K'], df_features['Stoch_D'] = compute_stochastic_oscillator(df_features['High'], df_features['Low'], df_features['Close'])
    df_features['ATR'] = compute_atr(df_features['High'], df_features['Low'], df_features['Close'])
    df_features['ROC'] = compute_roc(df_features['Close'])

    df_features['Close_Lag1'] = df_features['Close'].shift(1)
    df_features['RSI_Lag1'] = df_features['RSI'].shift(1)
    df_features['MACD_Lag1'] = df_features['MACD'].shift(1)
    df_features['Stoch_K_Lag1'] = df_features['Stoch_K'].shift(1)
    df_features['ATR_Lag1'] = df_features['ATR'].shift(1)
    df_features['ROC_Lag1'] = df_features['ROC'].shift(1)
    df_features['Volume_Lag1'] = df_features['Volume'].shift(1)

    df_features['Hour'] = df_features.index.hour
    df_features['DayOfWeek'] = df_features.index.dayofweek
    df_features['DayOfMonth'] = df_features.index.day
    df_features['Month'] = df_features.index.month

    df_features['PriceChange'] = df_features['Close'].pct_change()
    df_features = df_features[df_features['PriceChange'].abs() < 0.1] 

    initial_rows = len(df_features)
    df_features = df_features.dropna() 
    logger.info(f"After dropna: {len(df_features)} rows (dropped {initial_rows - len(df_features)})")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    required_rows = SEQUENCE_LENGTH + HORIZON_PERIODS
    if len(df_features) < required_rows:
        logger.error(f"Insufficient data: {len(df_features)} rows, required at least {required_rows} for sequences and horizon.")
        raise ValueError(f"Insufficient data: {len(df_features)} rows, required at least {required_rows}.")
    
    # –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ–º NaN, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è –∏–∑-–∑–∞ —Å–¥–≤–∏–≥–∞ Target
    df_features = df_features.dropna(subset=['Target'])

    feature_columns = [
        'Open', 'High', 'Low', 'Close', 
        'RSI', 'MA20', 'BB_Up', 'BB_Low', 'MACD', 'MACD_Sig',
        'Stoch_K', 'Stoch_D', 'ATR', 'ROC',
        'Close_Lag1', 'RSI_Lag1', 'MACD_Lag1', 'Stoch_K_Lag1', 'ATR_Lag1', 'ROC_Lag1',
        'Volume', 'Volume_Lag1',
        'Hour', 'DayOfWeek', 'DayOfMonth', 'Month'
    ]
    X_raw = df_features[feature_columns]
    y_raw = (df_features['Target'] > df_features['Close']).astype(int) # –¶–µ–ª—å: —Ü–µ–Ω–∞ —á–µ—Ä–µ–∑ HORIZON_PERIODS –≤—ã—à–µ —Ç–µ–∫—É—â–µ–π

    X_raw = X_raw.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(method='bfill')

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_raw)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X_raw.columns, index=X_raw.index)

    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    X_seq, y_seq = create_sequences(X_scaled_df, y_raw, SEQUENCE_LENGTH, HORIZON_PERIODS)

    logger.info(f"X_seq shape: {X_seq.shape}, y_seq shape: {y_seq.shape}, y_seq distribution: {pd.Series(y_seq).value_counts().to_dict()}")
    return X_seq, y_seq, scaler, df_features # df_features –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–π —Ü–µ–Ω—ã

def train_model():
    try:
        X_seq, y_seq, scaler, df_original = prepare_data()
    except Exception as e:
        logger.error(f"Data preparation error, cannot train model: {e}")
        return

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
    # –î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π train_test_split —Å shuffle=False –≤—Å–µ –µ—â–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
    # –µ—Å–ª–∏ –º—ã –Ω–µ –¥–µ–ª–∞–µ–º TimeSeriesSplit –≤–Ω—É—Ç—Ä–∏ Optuna objective.
    # –ù–æ –¥–ª—è TimeSeriesSplit –Ω—É–∂–Ω–æ –±—ã—Ç—å –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–º —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏.
    # X_seq —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø–æ—ç—Ç–æ–º—É TimeSeriesSplit –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –Ω–∏—Ö.
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ X_seq –∏ y_seq –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    if len(X_seq) != len(y_seq):
        raise ValueError("X_seq and y_seq must have the same number of samples.")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val –∏ test
    test_size_ratio = 0.2
    split_index = int(len(X_seq) * (1 - test_size_ratio))
    
    X_train_val, X_test = X_seq[:split_index], X_seq[split_index:]
    y_train_val, y_test = y_seq[:split_index], y_seq[split_index:]

    logger.info(f"Train/Validation set size: {len(X_train_val)}, Test set size: {len(X_test)}")

    # –£—á–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    neg_count = pd.Series(y_train_val).value_counts().get(0, 0)
    pos_count = pd.Series(y_train_val).value_counts().get(1, 0)
    class_weight = {0: 1.0, 1: neg_count / pos_count if pos_count > 0 else 1.0}
    logger.info(f"Class distribution in training data: Neg={neg_count}, Pos={pos_count}. Class weights={class_weight}")

    def objective(trial):
        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        n_layers = trial.suggest_int('n_layers', 1, 3)
        n_units = trial.suggest_int('n_units', 32, 256, step=32)
        activation = trial.suggest_categorical('activation', ['relu', 'tanh'])
        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])

        model = keras.Sequential()
        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: Flatten –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è 2D –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ 1D –≤–µ–∫—Ç–æ—Ä
        model.add(layers.Flatten(input_shape=(SEQUENCE_LENGTH, X_train_val.shape[2])))
        
        for i in range(n_layers):
            model.add(layers.Dense(n_units, activation=activation))
            model.add(layers.Dropout(dropout_rate))
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        model.add(layers.Dense(1, activation='sigmoid'))

        optimizer = None
        if optimizer_name == 'Adam':
            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        elif optimizer_name == 'RMSprop':
            optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)

        model.compile(optimizer=optimizer,
                      loss='binary_crossentropy',
                      metrics=['accuracy', tf.keras.metrics.F1Score(average='weighted', name='f1_score')])

        # TimeSeriesSplit –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        tscv = TimeSeriesSplit(n_splits=N_SPLITS_TS_CV)
        f1_scores = []

        for fold, (train_index, val_index) in enumerate(tscv.split(X_train_val)):
            X_fold_train, X_fold_val = X_train_val[train_index], X_train_val[val_index]
            y_fold_train, y_fold_val = y_train_val[train_index], y_train_val[val_index]

            # Callbacks –¥–ª—è Optuna Pruning –∏ Early Stopping
            callbacks = [
                TFKerasPruningCallback(trial, 'val_f1_score'), # –ü—Ä—É–Ω–∏–Ω–≥ –ø–æ F1-score
                EarlyStopping(monitor='val_f1_score', patience=NN_PATIENCE, mode='max', restore_best_weights=True)
            ]

            history = model.fit(X_fold_train, y_fold_train,
                                epochs=NN_EPOCHS,
                                batch_size=NN_BATCH_SIZE,
                                validation_data=(X_fold_val, y_fold_val),
                                callbacks=callbacks,
                                verbose=0, # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥ –æ–±—É—á–µ–Ω–∏—è
                                class_weight=class_weight) # –£—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤

            # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π F1-score –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ
            best_val_f1 = max(history.history['val_f1_score'])
            f1_scores.append(best_val_f1)

        avg_f1 = np.mean(f1_scores)
        
        trial.report(avg_f1, trial.number)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

        return avg_f1

    logger.info("üîç Starting Optuna hyperparameter search for Neural Network...")
    study = optuna.create_study(
        direction="maximize",
        pruner=optuna.pruners.HyperbandPruner(),
        sampler=optuna.samplers.TPESampler(),
        study_name=OPTUNA_STUDY_NAME,
        storage=OPTUNA_STORAGE_URL,
        load_if_exists=True
    )
    study.optimize(objective, n_trials=None, timeout=MAX_TRAINING_TIME)

    best_params = study.best_params
    best_f1_score = study.best_value

    logger.info(f"‚úÖ Optuna best F1-score: {best_f1_score:.4f}")
    logger.info(f"üìã Best params: {best_params}")

    if best_f1_score < MIN_ACCURACY_FOR_SIGNAL:
        logger.warning(f"‚ùå F1-score {best_f1_score:.2f} too low. No model saved.")
        return

    # –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    final_model = keras.Sequential()
    final_model.add(layers.Flatten(input_input_shape=(SEQUENCE_LENGTH, X_train_val.shape[2])))
    for i in range(best_params['n_layers']):
        final_model.add(layers.Dense(best_params['n_units'], activation=best_params['activation']))
        final_model.add(layers.Dropout(best_params['dropout_rate']))
    final_model.add(layers.Dense(1, activation='sigmoid'))

    optimizer = None
    if best_params['optimizer'] == 'Adam':
        optimizer = keras.optimizers.Adam(learning_rate=best_params['learning_rate'])
    elif best_params['optimizer'] == 'RMSprop':
        optimizer = keras.optimizers.RMSprop(learning_rate=best_params['learning_rate'])

    final_model.compile(optimizer=optimizer,
                        loss='binary_crossentropy',
                        metrics=['accuracy', tf.keras.metrics.F1Score(average='weighted', name='f1_score')])

    # Callbacks –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    callbacks_final = [
        EarlyStopping(monitor='val_f1_score', patience=NN_PATIENCE * 2, mode='max', restore_best_weights=True) # –£–≤–µ–ª–∏—á–∏–º patience
    ]

    # –†–∞–∑–¥–µ–ª—è–µ–º X_train_val –Ω–∞ train –∏ validation –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    final_train_split_index = int(len(X_train_val) * 0.8) # 80% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, 20% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    X_final_train, X_final_val = X_train_val[:final_train_split_index], X_train_val[final_train_split_index:]
    y_final_train, y_final_val = y_train_val[:final_train_split_index], y_train_val[final_train_split_index:]

    logger.info(f"Final model training on {len(X_final_train)} samples, validating on {len(X_final_val)} samples.")
    final_model.fit(X_final_train, y_final_train,
                    epochs=NN_EPOCHS,
                    batch_size=NN_BATCH_SIZE,
                    validation_data=(X_final_val, y_final_val),
                    callbacks=callbacks_final,
                    verbose=1,
                    class_weight=class_weight)

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ
    test_loss, test_accuracy, test_f1_score = final_model.evaluate(X_test, y_test, verbose=0)
    logger.info(f"Final model performance on TEST set: Accuracy={test_accuracy:.4f}, F1-score={test_f1_score:.4f}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Keras
    final_model.save(MODEL_PATH)
    joblib.dump({'scaler': scaler}, 'scaler.pkl') # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –æ—Ç–¥–µ–ª—å–Ω–æ, —Ç.–∫. –º–æ–¥–µ–ª—å Keras –Ω–µ —Ö—Ä–∞–Ω–∏—Ç –µ–≥–æ

    with open(ACCURACY_PATH, "w") as f:
        json.dump({
            "accuracy": float(test_accuracy), # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy.float32 –≤ float –¥–ª—è JSON
            "f1_score": float(test_f1_score), # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy.float32 –≤ float –¥–ª—è JSON
            "last_trained": str(datetime.now()),
            "best_params": best_params
        }, f)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
    # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞–º –Ω—É–∂–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    # X_seq —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø–æ—Å–ª–µ–¥–Ω—è—è –∏–∑ –Ω–∏—Ö - —Å–∞–º–∞—è –∞–∫—Ç—É–∞–ª—å–Ω–∞—è
    if X_seq.shape[0] > 0:
        # –°–∞–º–∞—è –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ X_seq
        latest_sequence_scaled = X_seq[-1:] 
        
        # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –µ–π "—Å—ã—Ä–∞—è" —Å—Ç—Ä–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
        # –≠—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞ –≤ df_features, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è X_seq
        # df_original - —ç—Ç–æ df_features –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        # –ù–∞–º –Ω—É–∂–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Ç–æ—á–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        # –≠—Ç–æ df_original.iloc[len(df_original) - 1]
        latest_original_data_point = df_original.iloc[-1:]

        generate_signal(final_model, scaler, latest_sequence_scaled, latest_original_data_point)
    else:
        logger.warning("No data to generate signal after training.")


def generate_signal(model, scaler, latest_sequence_scaled, latest_original_data_point):
    try:
        if latest_sequence_scaled.shape[0] == 0:
            logger.warning("No latest sequence data to generate signal.")
            return
        if latest_original_data_point.empty:
            logger.warning("No latest original data point to get current price.")
            return

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        # predict() –¥–ª—è Keras –º–æ–¥–µ–ª–µ–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç numpy array, –¥–∞–∂–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        prediction_proba = model.predict(latest_sequence_scaled)[0] # [0] —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Å–∞–º —Å–∫–∞–ª—è—Ä
        
        # current_price –±–µ—Ä–µ—Ç—Å—è –∏–∑ –Ω–µ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        current_price = latest_original_data_point['Close'].iloc[0] 
        
        signal_type = "HOLD"
        stop_loss = None
        take_profit = None

        # prediction_proba[0] - —ç—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1 (–¥–≤–∏–∂–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö)
        buy_probability = prediction_proba[0] 
        sell_probability = 1 - prediction_proba[0] 

        if buy_probability >= PREDICTION_PROB_THRESHOLD:
            signal_type = "BUY"
            stop_loss = current_price * 0.99
            take_profit = current_price * 1.015
        elif sell_probability >= PREDICTION_PROB_THRESHOLD: # –ò—Å–ø–æ–ª—å–∑—É–µ–º 1 - buy_probability –¥–ª—è SELL
            signal_type = "SELL"
            stop_loss = current_price * 1.01
            take_profit = current_price * 0.985
        
        signal = {
            "time": str(datetime.now()),
            "price": round(current_price, 5),
            "signal": signal_type,
            "buy_proba": round(buy_probability, 4),
            "sell_proba": round(sell_probability, 4),
            "stop_loss": round(stop_loss, 5) if stop_loss else "N/A",
            "take_profit": round(take_profit, 5) if take_profit else "N/A",
        }

        msg = (
            f"üìä Signal: {signal['signal']}\n"
            f"üïí Time: {signal['time']}\n"
            f"üí∞ Price: {signal['price']}\n"
            f"‚¨ÜÔ∏è Buy Proba: {signal['buy_proba']}\n"
            f"‚¨áÔ∏è Sell Proba: {signal['sell_proba']}\n"
            f"üìâ Stop Loss: {signal['stop_loss']}\n"
            f"üìà Take Profit: {signal['take_profit']}"
        )
        logger.info(f"Signal generated:\n{msg}")
        send_telegram_message(msg)
    except Exception as e:
        logger.error(f"Signal generation error: {e}")

@app.get("/")
async def root():
    if not os.path.exists(MODEL_PATH) or not os.path.exists(ACCURACY_PATH):
        logger.info("Model or accuracy file not found. Training new model.")
        train_model()
    else:
        with open(ACCURACY_PATH, "r") as f:
            data = json.load(f)
        last_trained = datetime.fromisoformat(data["last_trained"])
        
        current_metric = data.get("f1_score", data.get("accuracy", 0.0))
        
        if (datetime.now() - last_trained).days >= 1 or current_metric < TARGET_ACCURACY:
            logger.info(f"Model needs retraining. Last trained: {last_trained}, Metric: {current_metric:.2f}")
            train_model()
        else:
            logger.info(f"Model is up to date. Last trained: {last_trained}, Metric: {current_metric:.2f}")
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Keras –∏ scaler
                model = keras.models.load_model(MODEL_PATH)
                scaler_data = joblib.load('scaler.pkl')
                scaler = scaler_data['scaler']
                
                end_date = datetime.now()
                df_latest_full = yf.download("EURUSD=X", interval="15m", period="7d", end=end_date) 
                if isinstance(df_latest_full.columns, pd.MultiIndex):
                    df_latest_full.columns = [col[0] for col in df_latest_full.columns]
                
                if 'Volume' in df_latest_full.columns and df_latest_full['Volume'].sum() == 0:
                    logger.warning("Volume data for latest download is present but all values are zero.")
                elif 'Volume' not in df_latest_full.columns:
                    logger.warning("Volume column not found in latest downloaded data. It will not be used as a feature.")
                    df_latest_full['Volume'] = 0

                df_latest_full['Close'] = df_latest_full['Close'].fillna(method='ffill').fillna(method='bfill')
                
                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ prepare_data)
                df_latest_full['RSI'] = compute_rsi(df_latest_full['Close'])
                df_latest_full['MA20'] = df_latest_full['Close'].rolling(window=20).mean()
                df_latest_full['BB_Up'], df_latest_full['BB_Low'] = compute_bollinger_bands(df_latest_full['Close'])
                df_latest_full['MACD'], df_latest_full['MACD_Sig'] = compute_macd(df_latest_full['Close'])
                
                df_latest_full['Stoch_K'], df_latest_full['Stoch_D'] = compute_stochastic_oscillator(df_latest_full['High'], df_latest_full['Low'], df_latest_full['Close'])
                df_latest_full['ATR'] = compute_atr(df_latest_full['High'], df_latest_full['Low'], df_latest_full['Close'])
                df_latest_full['ROC'] = compute_roc(df_latest_full['Close'])

                df_latest_full['Close_Lag1'] = df_latest_full['Close'].shift(1)
                df_latest_full['RSI_Lag1'] = df_latest_full['RSI'].shift(1)
                df_latest_full['MACD_Lag1'] = df_latest_full['MACD'].shift(1)
                df_latest_full['Stoch_K_Lag1'] = df_latest_full['Stoch_K'].shift(1)
                df_latest_full['ATR_Lag1'] = df_latest_full['ATR'].shift(1)
                df_latest_full['ROC_Lag1'] = df_latest_full['ROC'].shift(1)
                df_latest_full['Volume_Lag1'] = df_latest_full['Volume'].shift(1)

                df_latest_full['Hour'] = df_latest_full.index.hour
                df_latest_full['DayOfWeek'] = df_latest_full.index.dayofweek
                df_latest_full['DayOfMonth'] = df_latest_full.index.day
                df_latest_full['Month'] = df_latest_full.index.month

                df_latest_full['PriceChange'] = df_latest_full['Close'].pct_change()
                df_latest_full = df_latest_full[df_latest_full['PriceChange'].abs() < 0.1]
                df_latest_full = df_latest_full.dropna()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                if len(df_latest_full) >= SEQUENCE_LENGTH:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    feature_columns = [
                        'Open', 'High', 'Low', 'Close', 
                        'RSI', 'MA20', 'BB_Up', 'BB_Low', 'MACD', 'MACD_Sig',
                        'Stoch_K', 'Stoch_D', 'ATR', 'ROC',
                        'Close_Lag1', 'RSI_Lag1', 'MACD_Lag1', 'Stoch_K_Lag1', 'ATR_Lag1', 'ROC_Lag1',
                        'Volume', 'Volume_Lag1',
                        'Hour', 'DayOfWeek', 'DayOfMonth', 'Month'
                    ]
                    latest_features_raw = df_latest_full[feature_columns].iloc[-SEQUENCE_LENGTH:]
                    latest_sequence_scaled = scaler.transform(latest_features_raw)
                    latest_sequence_scaled = np.expand_dims(latest_sequence_scaled, axis=0) # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ –±–∞—Ç—á–∞

                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é "—Å—ã—Ä—É—é" —Ç–æ—á–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
                    latest_original_data_point = df_latest_full.iloc[-1:]

                    generate_signal(model, scaler, latest_sequence_scaled, latest_original_data_point)
                else:
                    logger.warning("Could not get enough latest data to generate signal from existing model (need at least SEQUENCE_LENGTH rows).")
            except Exception as e:
                logger.error(f"Error loading model or generating signal from existing model: {e}")

    return {"status": "Bot is running"}

if __name__ == "__main__":
    uvicorn.run(app, host=config.UVICORN_HOST, port=config.UVICORN_PORT)
