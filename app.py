import yfinance as yf
import pandas as pd
import numpy as np
import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import f1_score, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
import json
from fastapi import FastAPI
import uvicorn
from datetime import datetime, timedelta
from send_telegram import send_telegram_message
import time
import optuna
import logging
import optuna.samplers
import config
import warnings
from threading import Thread
from optuna.exceptions import TrialPruned # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º TrialPruned

# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å UserWarning –∏–∑ Optuna, –µ—Å–ª–∏ –æ–Ω–∏ —Å–≤—è–∑–∞–Ω—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º —à–∞–≥–æ–≤
warnings.filterwarnings("ignore", message="The reported value is ignored because this `step` is already reported.", category=UserWarning)
# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å FutureWarning –æ—Ç sklearn.utils.deprecation
warnings.filterwarnings("ignore", category=FutureWarning)
# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å UserWarning –æ—Ç LightGBM, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å np.ndarray subset
warnings.filterwarnings("ignore", message="Usage of np.ndarray subset \(sliced data\) is not recommended due to it will double the peak memory cost in LightGBM.", category=UserWarning)
warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)


# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(level=config.LOG_LEVEL, format="""%(asctime)s - %(levelname)s - %(message)s""")
logger = logging.getLogger(__name__)

app = FastAPI()

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ config.py ---
MODEL_PATH = config.MODEL_PATH
ACCURACY_PATH = config.ACCURACY_PATH
HORIZON_PERIODS = config.HORIZON_PERIODS
LOOKBACK_PERIOD = config.LOOKBACK_PERIOD
MIN_DATA_ROWS = config.MIN_DATA_ROWS
TARGET_ACCURACY = config.TARGET_ACCURACY
MIN_ACCURACY_FOR_SIGNAL = config.MIN_ACCURACY_FOR_SIGNAL
MAX_TRAINING_TIME = config.MAX_TRAINING_TIME
PREDICTION_PROB_THRESHOLD = config.PREDICTION_PROB_THRESHOLD
N_SPLITS_TS_CV = config.N_SPLITS_TS_CV
OPTUNA_STORAGE_URL = config.OPTUNA_STORAGE_URL
OPTUNA_STUDY_NAME = config.OPTUNA_STUDY_NAME
TARGET_PRICE_CHANGE_THRESHOLD = config.TARGET_PRICE_CHANGE_THRESHOLD # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –∏–∑ config

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ---
def compute_rsi(data, periods=14):
    delta = data.diff()
    gain = delta.where(delta > 0, 0).rolling(window=periods).mean()
    loss = -delta.where(delta < 0, 0).rolling(window=periods).mean()
    rs = gain / (loss + 1e-10)
    rsi = 100 - (100 / (1 + rs))
    return rsi

def compute_bollinger_bands(data, window=20, num_std=2):
    ma = data.rolling(window=window).mean()
    std = data.rolling(window=window).std()
    upper = ma + (num_std * std)
    lower = ma - (num_std * std)
    return upper, lower

def compute_macd(data, fast=12, slow=26, signal=9):
    exp1 = data.ewm(span=fast, adjust=False).mean()
    exp2 = data.ewm(span=slow, adjust=False).mean()
    macd = exp1 - exp2
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    return macd, signal_line

def compute_stochastic_oscillator(high, low, close, k_period=14, d_period=3):
    lowest_low = low.rolling(window=k_period).min()
    highest_high = high.rolling(window=k_period).max()
    k_percent = ((close - lowest_low) / (highest_high - lowest_low + 1e-10)) * 100
    d_percent = k_percent.rolling(window=d_period).mean()
    return k_percent, d_percent

def compute_atr(high, low, close, period=14):
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = true_range.ewm(span=period, adjust=False).mean()
    return atr

def compute_roc(data, period=12):
    roc = ((data - data.shift(period)) / data.shift(period)) * 100
    return roc

def compute_adx(high, low, close, period=14):
    plus_dm = high.diff()
    minus_dm = low.diff()
    plus_dm[plus_dm < 0] = 0
    minus_dm[minus_dm > 0] = 0
    
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = true_range.ewm(span=period, adjust=False).mean()

    plus_di = (plus_dm.ewm(span=period, adjust=False).mean() / atr) * 100
    minus_di = (abs(minus_dm).ewm(span=period, adjust=False).mean() / atr) * 100
    
    dx = (abs(plus_di - minus_di) / (plus_di + minus_di + 1e-10)) * 100
    adx = dx.ewm(span=period, adjust=False).mean()
    return adx

def compute_psar(high, low, close, acceleration=0.02, maximum=0.2):
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    psar = close.copy() # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ PSAR
    ep = close.copy() # Extreme Point
    af = acceleration # Acceleration Factor
    
    bull = True # –§–ª–∞–≥ –±—ã—á—å–µ–≥–æ —Ç—Ä–µ–Ω–¥–∞
    
    for i in range(1, len(close)):
        if bull:
            psar[i] = psar[i-1] + af * (ep[i-1] - psar[i-1])
            if low[i] < psar[i]: # –†–∞–∑–≤–æ—Ä–æ—Ç —Ç—Ä–µ–Ω–¥–∞
                bull = False
                psar[i] = ep[i-1]
                af = acceleration
                ep[i] = low[i]
            else:
                if high[i] > ep[i-1]:
                    af = min(af + acceleration, maximum)
                    ep[i] = high[i]
                else:
                    ep[i] = ep[i-1]
        else: # –ú–µ–¥–≤–µ–∂–∏–π —Ç—Ä–µ–Ω–¥
            psar[i] = psar[i-1] - af * (psar[i-1] - ep[i-1])
            if high[i] > psar[i]: # –†–∞–∑–≤–æ—Ä–æ—Ç —Ç—Ä–µ–Ω–¥–∞
                bull = True
                psar[i] = ep[i-1]
                af = acceleration
                ep[i] = high[i]
            else:
                if low[i] < ep[i-1]:
                    af = min(af + acceleration, maximum)
                    ep[i] = low[i]
                else:
                    ep[i] = ep[i-1]
    return psar

# --- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞ F1-score –¥–ª—è LightGBM (–¥–ª—è LGBMClassifier) ---
def lgbm_f1_score(y_pred, y_true):
    y_true_binary = y_true.astype(int)
    y_pred_binary = (y_pred > 0.5).astype(int) # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    return 'f1_score', f1_score(y_true_binary, y_pred_binary, average='weighted'), True # True –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ

# --- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞ F1-score –¥–ª—è LightGBM (–¥–ª—è lightgbm.cv) ---
def lgbm_f1_score_for_cv(preds, train_data):
    labels = train_data.get_label()
    y_pred_binary = (preds > 0.5).astype(int)
    return 'f1_score', f1_score(labels, y_pred_binary, average='weighted'), True


def download_and_process_data(interval, period, end_date, max_retries=5, initial_delay=5):
    logger.info(f"Downloading data for interval {interval} with period {period}...")
    for attempt in range(max_retries):
        try:
            df = yf.download("EURUSD=X", interval=interval, period=period, end=end_date)
            if df.empty:
                logger.warning(f"Empty data received for interval {interval}. No data to process.")
                # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ –∏–∑-–∑–∞ –ª–∏–º–∏—Ç–∞, –∞ –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö.
                # –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞ –ø–æ–≤—Ç–æ—Ä—è—Ç—å, –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None.
                return None
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = [col[0] for col in df.columns]
            df = df[(df["Close"] > 0) & (df["Open"] > 0)]
            df["Close"] = df["Close"].ffill().bfill()

            if df.index.tz is not None:
                df.index = df.index.tz_localize(None)

            return df
        except Exception as e:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—à–∏–±–∫–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –ª–∏–º–∏—Ç–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
            if "Rate limited" in str(e) or isinstance(e, yf.YFRateLimitError):
                delay = initial_delay * (2**attempt) # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                logger.warning(f"Rate limit hit for {interval} data. Retrying in {delay} seconds (attempt {attempt + 1}/{max_retries})...")
                time.sleep(delay)
            else:
                logger.error(f"Error downloading data for interval {interval}: {str(e)}")
                return None # –ï—Å–ª–∏ –¥—Ä—É–≥–∞—è –æ—à–∏–±–∫–∞, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º
    logger.error(f"Failed to download data for interval {interval} after {max_retries} attempts due to rate limiting.")
    return None


def calculate_indicators(df):
    if df is None or df.empty:
        return pd.DataFrame()
    df_ind = df.copy()
    df_ind["RSI"] = compute_rsi(df_ind["Close"])
    df_ind["MA20"] = df_ind["Close"].rolling(window=20).mean()
    df_ind["BB_Up"], df_ind["BB_Low"] = compute_bollinger_bands(df_ind["Close"])
    df_ind["MACD"], df_ind["MACD_Sig"] = compute_macd(df_ind["Close"])
    df_ind["Stoch_K"], df_ind["Stoch_D"] = compute_stochastic_oscillator(df_ind["High"], df_ind["Low"], df_ind["Close"])
    df_ind["ATR"] = compute_atr(df_ind["High"], df_ind["Low"], df_ind["Close"])
    df_ind["ROC"] = compute_roc(df_ind["Close"])
    df_ind["ADX"] = compute_adx(df_ind["High"], df_ind["Low"], df_ind["Close"])
    df_ind["PSAR"] = compute_psar(df_ind["High"], df_ind["Low"], df_ind["Close"])
    return df_ind

def prepare_data():
    end_date = datetime.now()

    # --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ ---
    df_15m = download_and_process_data("15m", LOOKBACK_PERIOD, end_date)
    if df_15m is None or df_15m.empty:
        raise ValueError("Failed to download or process 15m data.")
    logger.info(f"Downloaded {len(df_15m)} rows for 15m interval.")
    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    time.sleep(2) 
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 1-—á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–≥–æ—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    df_1h = download_and_process_data("1h", "120d", end_date) # 120 –¥–Ω–µ–π –¥–ª—è 1-—á–∞—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    # --- 2. –†–∞—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ ---
    df_15m_features = calculate_indicators(df_15m)
    df_1h_features = calculate_indicators(df_1h)

    # --- 3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ ---
    df_list = [df_15m_features]

    indicators_to_merge = ["RSI", "MA20", "BB_Up", "BB_Low", "MACD", "MACD_Sig",
                           "Stoch_K", "Stoch_D", "ATR", "ROC", "ADX", "PSAR"]

    # Resample and add multi-timeframe indicators (—Ç–æ–ª—å–∫–æ –¥–ª—è 1h)
    if df_1h_features is not None and not df_1h_features.empty:
        for ind in indicators_to_merge:
            if ind in df_1h_features.columns:
                resampled_df = df_1h_features[[ind]].resample("15min").ffill().rename(columns={ind: f"{ind}_1h"})
                df_list.append(resampled_df)
    
    df_combined = pd.concat(df_list, axis=1)
    df_combined = df_combined[~df_combined.index.duplicated(keep='first')]
    df_combined = df_combined.sort_index()

    # --- 4. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∞–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
    lag_periods = [1, 2, 3, 4]
    lagged_features_df = pd.DataFrame(index=df_combined.index)

    cols_to_lag = [col for col in df_combined.columns if col not in ["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits"]]
    cols_to_lag.extend(["Close"])

    for col in set(cols_to_lag):
        if col in df_combined.columns:
            for lag in lag_periods:
                lagged_features_df[f"{col}_Lag{lag}"] = df_combined[col].shift(lag)
    
    df_combined = pd.concat([df_combined, lagged_features_df], axis=1)

    # --- 5. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
    time_features_df = pd.DataFrame(index=df_combined.index)
    time_features_df["Hour"] = df_combined.index.hour
    time_features_df["DayOfWeek"] = df_combined.index.dayofweek
    time_features_df["DayOfMonth"] = df_combined.index.day
    time_features_df["Month"] = df_combined.index.month
    df_combined = pd.concat([df_combined, time_features_df], axis=1)

    # --- 6. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ---
    df_combined["Target_Raw"] = df_combined["Close"].shift(-HORIZON_PERIODS)
    
    df_combined["Target"] = np.nan
    df_combined.loc[df_combined["Target_Raw"] > df_combined["Close"] * (1 + TARGET_PRICE_CHANGE_THRESHOLD), "Target"] = 1
    df_combined.loc[df_combined["Target_Raw"] < df_combined["Close"] * (1 - TARGET_PRICE_CHANGE_THRESHOLD), "Target"] = 0

    # --- 7. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
    df_combined["PriceChange"] = df_combined["Close"].pct_change()
    df_combined = df_combined[df_combined["PriceChange"].abs() < 0.1]
    
    initial_rows = len(df_combined)
    nan_in_target = df_combined["Target"].isna().sum()
    logger.info(f"Rows with NaN in Target before dropna: {nan_in_target}")

    df_combined = df_combined.dropna()
    logger.info(f"After dropna: {len(df_combined)} rows (dropped {initial_rows - len(df_combined)})")

    if len(df_combined) < MIN_DATA_ROWS:
        logger.error(f"Insufficient data: {len(df_combined)} rows, required at least {MIN_DATA_ROWS}.")
        raise ValueError(f"Insufficient data: {len(df_combined)} rows, required at least {MIN_DATA_ROWS}.")
    
    # --- 8. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ X –∏ y ---
    exclude_cols = ["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits", 
                    "Target_Raw", "Target", "PriceChange"]
    
    feature_columns = [col for col in df_combined.columns if col not in exclude_cols and not col.startswith("Adj Close")]
    
    for basic_col in ["Open", "High", "Low", "Close"]:
        if basic_col in df_combined.columns and basic_col not in feature_columns:
            feature_columns.append(basic_col)

    X_raw = df_combined[feature_columns]
    y_raw = df_combined["Target"].astype(int)

    X_raw = X_raw.replace([np.inf, -np.inf], np.nan).ffill().bfill()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_raw)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X_raw.columns, index=X_raw.index)

    logger.info(f"X shape: {X_scaled_df.shape}, y shape: {y_raw.shape}, y distribution: {y_raw.value_counts().to_dict()}")
    return X_scaled_df, y_raw, scaler, df_combined

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è X_train_val, y_train_val, scale_pos_weight
# –û–Ω–∏ –±—É–¥—É—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤ train_model –∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ objective
X_train_val = None
y_train_val = None
scale_pos_weight = 1.0

def objective(trial):
    global X_train_val, y_train_val, scale_pos_weight # –û–±—ä—è–≤–ª—è–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ

    logger.info(f"üöÄ Trial #{trial.number} started") # –ù–û–í–´–ô –õ–û–ì
    params = {
        "objective": "binary",
        "metric": "binary_logloss",  # —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –Ω—É–∂–µ–Ω, –Ω–æ –º—ã –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–º —á–µ—Ä–µ–∑ feval
        "n_estimators": trial.suggest_int("n_estimators", 50, 300), # –£–º–µ–Ω—å—à–µ–Ω –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "num_leaves": trial.suggest_int("num_leaves", 10, 50), # –£–º–µ–Ω—å—à–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
        "max_depth": trial.suggest_int("max_depth", 3, 7), # –£–º–µ–Ω—å—à–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω
        "min_child_samples": trial.suggest_int("min_child_samples", 20, 100),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 1.0, log=True),
        "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 1.0, log=True),
        "random_state": 42,
        "n_jobs": -1,
        "verbose": -1,
        "boosting_type": "gbdt",
        "scale_pos_weight": scale_pos_weight
    }
    
    lgb_train = lgb.Dataset(X_train_val, y_train_val)
    folds = TimeSeriesSplit(n_splits=N_SPLITS_TS_CV) # N_SPLITS_TS_CV –∏–∑ config.py
    
    logger.info(f"Trial #{trial.number}: Starting LightGBM CV with {params['n_estimators']} estimators and {N_SPLITS_TS_CV} folds...") # –ù–û–í–´–ô –õ–û–ì
    start_cv_time = time.time()
    try:
        cv_results = lgb.cv(
            params,
            lgb_train,
            num_boost_round=params["n_estimators"],
            folds=folds,
            feval=lgbm_f1_score_for_cv,
            stratified=False,
            return_cvbooster=False,
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
                optuna.integration.LightGBMPruningCallback(trial, "valid f1_score") # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ "valid f1_score"
            ]
        )
        logger.info(f"Trial #{trial.number}: LightGBM CV completed.") # –ù–û–í–´–ô –õ–û–ì
        logger.info(f"‚è± Trial #{trial.number} CV time: {time.time() - start_cv_time:.2f} seconds") # –ù–û–í–´–ô –õ–û–ì

        try:
            avg_f1 = cv_results['cv_agg f1_score-mean'][-1]
        except KeyError:
            # Fallback, –µ—Å–ª–∏ –∏–º—è –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è (—Ö–æ—Ç—è –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å)
            avg_f1 = cv_results['cv_agg f1_score-mean'][-1] 
            logger.warning(f"Trial #{trial.number}: Used fallback metric name for F1-score.")
            
        logger.info(f"Trial #{trial.number}: Average F1-score = {avg_f1:.4f}") # –ù–û–í–´–ô –õ–û–ì
        
        return avg_f1

    except Exception as e:
        logger.error(f"Error during LightGBM CV for trial #{trial.number}: {e}")
        # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è CV, –º–æ–∂–Ω–æ –ø—Ä—É–Ω–∏—Ç—å –ø—Ä–æ–±–Ω—ã–π –∑–∞–ø—É—Å–∫
        raise TrialPruned() # –≠—Ç–æ —Å–æ–æ–±—â–∏—Ç Optuna, —á—Ç–æ –ø—Ä–æ–±–Ω—ã–π –∑–∞–ø—É—Å–∫ –Ω–µ—É–¥–∞—á–µ–Ω

def train_model():
    global X_train_val, y_train_val, scale_pos_weight # –û–±—ä—è–≤–ª—è–µ–º, —á—Ç–æ –±—É–¥–µ–º –∏–∑–º–µ–Ω—è—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ

    logger.info("Starting model training process...")
    try:
        X, y, scaler, df_original = prepare_data()
    except Exception as e:
        logger.error(f"Data preparation error, cannot train model: {e}")
        return

    if len(X) != len(y):
        logger.error("X and y must have the same number of samples.")
        return

    X_train_val_local, X_test, y_train_val_local, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False, stratify=None
    )
    
    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö
    X_train_val = X_train_val_local
    y_train_val = y_train_val_local

    logger.info(f"Train/Validation set size: {len(X_train_val)}, Test set size: {len(X_test)}")

    neg_count = y_train_val.value_counts().get(0, 0)
    pos_count = y_train_val.value_counts().get(1, 0)
    
    scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0
    logger.info(f"Class distribution in training data: Neg={neg_count}, Pos={pos_count}. Scale_pos_weight={scale_pos_weight}")

    logger.info("üîç Starting Optuna hyperparameter search for LightGBM...")
    logger.info(f"Optuna storage URL: {OPTUNA_STORAGE_URL}") # –ù–æ–≤—ã–π –ª–æ–≥
    logger.info(f"Optuna study name: {OPTUNA_STUDY_NAME}") # –ù–æ–≤—ã–π –ª–æ–≥

    try:
        study = optuna.create_study(
            direction="maximize",
            pruner=optuna.pruners.HyperbandPruner(),
            sampler=optuna.samplers.TPESampler(),
            study_name=OPTUNA_STUDY_NAME,
            storage=OPTUNA_STORAGE_URL,
            load_if_exists=True
        )
        logger.info("Optuna study created/loaded successfully.") # –ù–æ–≤—ã–π –ª–æ–≥
    except Exception as e:
        logger.error(f"Error creating/loading Optuna study: {e}")
        return

    try:
        study.optimize(objective, n_trials=None, timeout=MAX_TRAINING_TIME)
        logger.info("Optuna optimization completed.") # –ù–æ–≤—ã–π –ª–æ–≥
    except Exception as e:
        logger.error(f"Error during Optuna optimization: {e}")
        return

    best_params = study.best_params
    best_f1_score = study.best_value

    logger.info(f"‚úÖ Optuna best F1-score: {best_f1_score:.4f}")
    logger.info(f"üìã Best params: {best_params}")

    if best_f1_score < MIN_ACCURACY_FOR_SIGNAL:
        logger.warning(f"‚ùå F1-score {best_f1_score:.2f} too low. No model saved.")
        return

    final_model = LGBMClassifier(**best_params, random_state=42, n_jobs=-1, verbose=-1)
    final_model.fit(X_train_val, y_train_val)
    logger.info("üéØ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    y_pred_test = final_model.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_f1_score = f1_score(y_test, y_pred_test, average='weighted')
    logger.info(f"Final model performance on TEST set: Accuracy={test_accuracy:.4f}, F1-score={test_f1_score:.4f}")

    joblib.dump(final_model, MODEL_PATH)
    joblib.dump({'scaler': scaler}, 'scaler.pkl')

    with open(ACCURACY_PATH, "w") as f:
        json.dump({
            "accuracy": float(test_accuracy),
            "f1_score": float(test_f1_score),
            "last_trained": str(datetime.now()),
            "best_params": best_params
        }, f)

    if len(X) > 0:
        # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        # df_original —Ç–µ–ø–µ—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –≤–∫–ª—é—á–∞—è –º–Ω–æ–≥–æ—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤—ã–µ –∏ –ª–∞–≥–∏
        # –∏ —É–∂–µ –æ—á–∏—â–µ–Ω –æ—Ç NaN.
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –∏–∑ X (–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
        latest_features_scaled = X.iloc[-1:].values
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É –∏–∑ df_original (–∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ + –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
        latest_original_data_point = df_original.iloc[-1:]

        generate_signal(final_model, scaler, latest_features_scaled, latest_original_data_point)
    else:
        logger.warning("No data to generate signal after training.")

def generate_signal(model, scaler, latest_features_scaled, latest_original_data_point):
    try:
        if latest_features_scaled.shape[0] == 0:
            logger.warning("No latest features data to generate signal.")
            return
        if latest_original_data_point.empty:
            logger.warning("No latest original data point to get current price.")
            return

        prediction_proba = model.predict_proba(latest_features_scaled)[0]
        
        current_price = latest_original_data_point['Close'].iloc[0] 
        
        signal_type = "HOLD"
        stop_loss = None
        take_profit = None

        buy_probability = prediction_proba[1]
        sell_probability = prediction_proba[0]

        if buy_probability >= PREDICTION_PROB_THRESHOLD:
            signal_type = "BUY"
            # –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞ SL/TP (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
            stop_loss = current_price * (1 - 0.001) # 0.1% –Ω–∏–∂–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
            take_profit = current_price * (1 + 0.002) # 0.2% –≤—ã—à–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
        elif sell_probability >= PREDICTION_PROB_THRESHOLD:
            signal_type = "SELL"
            stop_loss = current_price * (1 + 0.001) # 0.1% –≤—ã—à–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
            take_profit = current_price * (1 - 0.002) # 0.2% –Ω–∏–∂–µ —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã

        message = (
            f"üìà Trading Signal: {signal_type}\n"
            f"Current Price: {current_price:.5f}\n"
            f"Buy Probability: {buy_probability:.4f}\n"
            f"Sell Probability: {sell_probability:.4f}"
        )
        if stop_loss and take_profit:
            message += f"\nStop Loss: {stop_loss:.5f}\nTake Profit: {take_profit:.5f}"

        logger.info(message)
        send_telegram_message(message)

    except Exception as e:
        logger.error(f"Error generating signal: {e}")


@app.on_event("startup")
def startup_event():
    Thread(target=train_model).start()

@app.get("/")
async def root():
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –∏ —Ñ–∞–π–ª —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é
    if not os.path.exists(MODEL_PATH) or not os.path.exists(ACCURACY_PATH):
        logger.info("Model or accuracy file not found. Training process should be running in background.")
        return {"status": "Model training in progress or not yet started."}

    try:
        with open(ACCURACY_PATH, "r") as f:
            accuracy_data = json.load(f)
        current_f1_score = accuracy_data.get("f1_score", 0.0)
        last_trained = datetime.fromisoformat(accuracy_data["last_trained"])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å
        # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª–µ–µ 1 –¥–Ω—è –∏–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ —Ü–µ–ª–µ–≤–æ–π
        if (datetime.now() - last_trained).days >= 1 or current_f1_score < TARGET_ACCURACY:
            logger.info(f"Model needs retraining. Last trained: {last_trained}, F1-score: {current_f1_score:.4f}. Starting retraining in background.")
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫ FastAPI
            Thread(target=train_model).start()
            return {"status": "Model retraining initiated in background."}

        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∞–∫—Ç—É–∞–ª—å–Ω–∞ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª
        if current_f1_score >= MIN_ACCURACY_FOR_SIGNAL:
            model = joblib.load(MODEL_PATH)
            scaler_data = joblib.load('scaler.pkl')
            scaler = scaler_data['scaler']
            
            end_date = datetime.now()
            df_latest_full_15m = download_and_process_data("15m", "7d", end_date)
            if df_latest_full_15m is None:
                logger.error("Failed to download 15m data for signal generation.")
                return {"status": "Error: Could not download latest 15m data for signal."}

            time.sleep(2) # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            df_latest_full_1h = download_and_process_data("1h", "120d", end_date)
            if df_latest_full_1h is None:
                logger.error("Failed to download 1h data for signal generation.")
                return {"status": "Error: Could not download latest 1h data for signal."}

            # –†–∞—Å—á–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            df_latest_full_15m_features = calculate_indicators(df_latest_full_15m)
            df_latest_full_1h_features = calculate_indicators(df_latest_full_1h)

            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            df_list_latest = [df_latest_full_15m_features]
            indicators_to_merge = ["RSI", "MA20", "BB_Up", "BB_Low", "MACD", "MACD_Sig",
                                   "Stoch_K", "Stoch_D", "ATR", "ROC", "ADX", "PSAR"]

            if df_latest_full_1h_features is not None and not df_latest_full_1h_features.empty:
                for ind in indicators_to_merge:
                    if ind in df_latest_full_1h_features.columns:
                        resampled_df = df_latest_full_1h_features[[ind]].resample("15min").ffill().rename(columns={ind: f"{ind}_1h"})
                        df_list_latest.append(resampled_df)

            df_combined_latest = pd.concat(df_list_latest, axis=1)
            df_combined_latest = df_combined_latest[~df_combined_latest.index.duplicated(keep='first')]
            df_combined_latest = df_combined_latest.sort_index()

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∞–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            lag_periods = [1, 2, 3, 4]
            lagged_features_df_latest = pd.DataFrame(index=df_combined_latest.index)
            cols_to_lag_latest = [col for col in df_combined_latest.columns if col not in ["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits"]]
            cols_to_lag_latest.extend(["Close"])

            for col in set(cols_to_lag_latest):
                if col in df_combined_latest.columns:
                    for lag in lag_periods:
                        lagged_features_df_latest[f"{col}_Lag{lag}"] = df_combined_latest[col].shift(lag)
            df_combined_latest = pd.concat([df_combined_latest, lagged_features_df_latest], axis=1)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            df_combined_latest["Hour"] = df_combined_latest.index.hour
            df_combined_latest["DayOfWeek"] = df_combined_latest.index.dayofweek
            df_combined_latest["DayOfMonth"] = df_combined_latest.index.day
            df_combined_latest["Month"] = df_combined_latest.index.month

            # –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ prepare_data)
            df_combined_latest["PriceChange"] = df_combined_latest["Close"].pct_change()
            df_combined_latest = df_combined_latest[df_combined_latest['PriceChange'].abs() < 0.1]
            df_combined_latest = df_combined_latest.dropna()

            if len(df_combined_latest) >= 1:
                # –ò—Å–∫–ª—é—á–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ prepare_data)
                exclude_cols_signal = ["Open", "High", "Low", "Close", "Volume", "Dividends", "Stock Splits", 
                                       "Target_Raw", "Target", "PriceChange"]
                
                feature_columns_signal = [col for col in df_combined_latest.columns if col not in exclude_cols_signal and not col.startswith("Adj Close")]
                
                for basic_col in ["Open", "High", "Low", "Close"]:
                    if basic_col in df_combined_latest.columns and basic_col not in feature_columns_signal:
                        feature_columns_signal.append(basic_col)

                latest_features_raw = df_combined_latest[feature_columns_signal].iloc[-1:]
                latest_features_scaled = scaler.transform(latest_features_raw)
                
                latest_original_data_point = df_combined_latest.iloc[-1:]

                generate_signal(model, scaler, latest_features_scaled, latest_original_data_point)
            else:
                logger.warning("Could not get enough latest data to generate signal from existing model (need at least 1 row after feature engineering).")
        else:
            logger.warning(f"Model F1-score {current_f1_score:.4f} is below MIN_ACCURACY_FOR_SIGNAL ({MIN_ACCURACY_FOR_SIGNAL}). No signal generated.")
            return {"status": "Model accuracy too low for signal generation."}

    except Exception as e:
        logger.error(f"Error loading model or generating signal from existing model: {e}")
        return {"status": f"Error: {e}"}

    return {"status": "Bot is running"}

if __name__ == "__main__":
    uvicorn.run(app, host=config.UVICORN_HOST, port=config.UVICORN_PORT)
